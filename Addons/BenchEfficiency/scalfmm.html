<h1 id="scalfmm-with-starpucuda">ScalFMM with StarPU+CUDA</h1>
<h2 id="installing-the-libraries">Installing the libraries</h2>
<p>For some installation steps, we provide a &quot;valid-if&quot; command which provide a basic test to ensure it should work. In case of success <code>STEP-OK</code> will be print-out. In addition, if a library is already installed on the system, it is possible to set the output variables directly and test with the &quot;valid-if&quot; command if it should work.</p>
<p>The installation and configuration to have the execution traces and executions times are marked as <strong>Optional</strong> but higly recommended since they let have the efficiencies. However, if one wants to execute without any overhead, it might need to remove the usage of FXT.</p>
<h3 id="pre-requiste">Pre-requiste:</h3>
<p>In order to follow this tutorial, it is needed to have the following applications installed:</p>
<ul>
<li>autoconf (&gt;= 2.69)</li>
<li>gawk (Awk &gt;= 4.0.1)</li>
<li>make (&gt;= 3.81)</li>
<li>cmake (&gt;= 3.2.2)</li>
<li>gcc/g++ (&gt;= 4.9) and the gcc/g++ names should point to the correct binaries</li>
<li>BLAS/LAPACK (The configure of ScalFMM is different if the MKL is used or not, but with the MKL it is recommended to set environment variable <code>MKLROOT</code>)</li>
<li>CUDA (&gt;= 7) and <code>CUDA_PATH</code> must be set. In our case, <code>CUDA_PATH=/usr/local/cuda-7.5/</code></li>
<li><strong>Optional</strong> Vite (from <code>sudo apt-get install vite</code> or see <a href="http://vite.gforge.inria.fr/download.php" class="uri">http://vite.gforge.inria.fr/download.php</a>)</li>
</ul>
<blockquote>
<p>Some installations of CUDA does not have libcuda file. In this case, one needs to create a link : <code>sudo ln /usr/local/cuda-7.5/lib64/libcudart.so /usr/local/cuda-7.5/lib64/libcuda.so</code></p>
</blockquote>
<blockquote>
<p>[Plafrim-Developers]</p>
<p>Alloc a node : salloc -N 1 --time=03:00:00 --exclusive -p court_sirocco -CHaswell --gres=gpu:4 -x sirocco06</p>
<p>Find it: squeue and ssh on it</p>
<p>Load modules : module load compiler/gcc/4.9.2 cuda75/toolkit/7.5.18 intel/mkl/64/11.2/2016.0.0 build/cmake/3.2.1</p>
</blockquote>
<h3 id="working-directory">Working directory</h3>
<p>The variable <code>SCALFMM_TEST_DIR</code> is used to specify the working directory:</p>
<pre class="bash"><code>export SCALFMM_TEST_DIR=~/scalfmm_test
mkdir $SCALFMM_TEST_DIR
cd $SCALFMM_TEST_DIR</code></pre>
<p><em>Output variables:</em> <code>$SCALFMM_TEST_DIR</code></p>
<p>Valid-if</p>
<pre class="bash"><code>if [[ -n $SCALFMM_TEST_DIR ]] &amp;&amp; [[ -d $SCALFMM_TEST_DIR ]] ; then
   echo “STEP-OK”
fi</code></pre>
<h3 id="downloading-packages">Downloading Packages</h3>
<p>In case the node used for compiling/testing do not have access to internet, then download the following packages first.</p>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
wget https://www.open-mpi.org/software/hwloc/v1.11/downloads/hwloc-1.11.2.tar.gz
wget http://download.savannah.gnu.org/releases/fkt/fxt-0.2.11.tar.gz # Optional
wget http://www.fftw.org/fftw-3.3.4.tar.gz
svn export svn://scm.gforge.inria.fr/svnroot/starpu/trunk starpu
git clone --depth=1 https://scm.gforge.inria.fr/anonscm/git/scalfmm-public/scalfmm-public.git</code></pre>
<h3 id="hwloc">HWLOC</h3>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
if [[ ! -f hwloc-1.11.2.tar.gz ]] ; then
    wget https://www.open-mpi.org/software/hwloc/v1.11/downloads/hwloc-1.11.2.tar.gz
fi
tar xvf hwloc-1.11.2.tar.gz
cd hwloc-1.11.2/
SCALFMM_HWLOC_DIR=$SCALFMM_TEST_DIR/hwlocinstall
./configure --prefix=$SCALFMM_HWLOC_DIR
make install</code></pre>
<p><em>Output variables:</em> <code>$SCALFMM_HWLOC_DIR</code></p>
<p>Valid-if:</p>
<pre class="bash"><code>if [[ -n $SCALFMM_HWLOC_DIR ]] &amp;&amp; [[ -d $SCALFMM_HWLOC_DIR/lib/ ]] &amp;&amp; [[ -f  $SCALFMM_HWLOC_DIR/lib/libhwloc.so ]]; then
   echo “OK”
fi</code></pre>
<h3 id="fxt-optional">FXT (<strong>Optional</strong>)</h3>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
if [[ ! -f fxt-0.2.11.tar.gz ]] ; then
    wget http://download.savannah.gnu.org/releases/fkt/fxt-0.2.11.tar.gz
fi
tar xvf fxt-0.2.11.tar.gz
cd fxt-0.2.11/
SCALFMM_FXT_DIR=$SCALFMM_TEST_DIR/fxtinstall
./configure --prefix=$SCALFMM_FXT_DIR
make install</code></pre>
<p><em>Output variables:</em> <code>$SCALFMM_FXT_DIR</code></p>
<p>Valid-if:</p>
<pre class="bash"><code>if [[ -n $SCALFMM_FXT_DIR ]] &amp;&amp; [[ -d $SCALFMM_FXT_DIR/lib/ ]] &amp;&amp; [[ -f  $SCALFMM_FXT_DIR/lib/libfxt.so ]]; then
   echo “OK”
fi</code></pre>
<h3 id="fftw">FFTW</h3>
<p>The MKL can be used otherwise we need the FFTW lib:</p>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
if [[ ! -f fftw-3.3.4.tar.gz ]] ; then
    wget http://www.fftw.org/fftw-3.3.4.tar.gz
fi    
tar xvf fftw-3.3.4.tar.gz
cd fftw-3.3.4/
SCALFMM_FFTW_DIR=$SCALFMM_TEST_DIR/fftinstall
./configure --prefix=$SCALFMM_FFTW_DIR
make install
./configure --prefix=$SCALFMM_FFTW_DIR --enable-float
make install</code></pre>
<p><em>Output variables:</em> <code>$SCALFMM_FFTW_DIR</code></p>
<p>Valid-if:</p>
<pre class="bash"><code>if [[ -n $SCALFMM_FFTW_DIR ]] &amp;&amp; [[ -d $SCALFMM_FFTW_DIR/lib/ ]] &amp;&amp; [[ -f  $SCALFMM_FFTW_DIR/lib/libfftw3.a ]] &amp;&amp; [[ -f  $SCALFMM_FFTW_DIR/lib/libfftw3f.a ]]; then
   echo “OK”
fi</code></pre>
<h3 id="starpu">StarPU</h3>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
if [[ ! -d starpu ]] ; then
	svn export svn://scm.gforge.inria.fr/svnroot/starpu/trunk starpu
fi    
cd starpu/
SCALFMM_STARPU_DIR=$SCALFMM_TEST_DIR/starpuinstall
./autogen.sh
./configure --prefix=$SCALFMM_STARPU_DIR --with-fxt=$SCALFMM_FXT_DIR --with-hwloc=$SCALFMM_HWLOC_DIR --with-cuda-dir=$CUDA_PATH --disable-opencl
make install</code></pre>
<blockquote>
<p><strong>Optional</strong> In case you do not want to use trace (FXT) please remove the <code>--with-fxt=$SCALFMM_FXT_DIR</code> parameter from the command</p>
</blockquote>
<p><em>Output variables:</em> <code>$SCALFMM_STARPU_DIR</code></p>
<p>Valid-if:</p>
<pre class="bash"><code>if [[ -n $SCALFMM_STARPU_DIR ]] &amp;&amp; [[ -d $SCALFMM_STARPU_DIR/lib/ ]] &amp;&amp; [[ -f  $SCALFMM_STARPU_DIR/lib/libstarpu.so ]] ; then
   echo “OK”
fi</code></pre>
<h3 id="scalfmm">ScalFMM</h3>
<h4 id="configure">Configure</h4>
<ul>
<li><p>Getting the source from the last commit:</p>
<pre class="bash"><code>cd $SCALFMM_TEST_DIR
if [[ ! -d scalfmm-public ]] ; then
git clone --depth=1 https://scm.gforge.inria.fr/anonscm/git/scalfmm-public/scalfmm-public.git
fi    
cd scalfmm-public/
export SCALFMM_SOURCE_DIR=`pwd`
Build/
export SCALFMM_BUILD_DIR=`pwd`</code></pre></li>
</ul>
<p><em>Output variables:</em> <code>SCALFMM_BUILD_DIR</code> <code>SCALFMM_SOURCE_DIR</code></p>
<ul>
<li><p>Configure (No MKL):</p>
<pre class="bash"><code>cmake .. -DSCALFMM_BUILD_DEBUG=OFF -DSCALFMM_USE_MPI=OFF -DSCALFMM_BUILD_TESTS=ON -DSCALFMM_BUILD_UTESTS=OFF -DSCALFMM_USE_BLAS=ON -DSCALFMM_USE_MKL_AS_BLAS=OFF -DSCALFMM_USE_LOG=ON -DSCALFMM_USE_STARPU=ON -DSCALFMM_USE_CUDA=ON -DSCALFMM_USE_OPENCL=OFF -DHWLOC_DIR=$SCALFMM_HWLOC_DIR -DSTARPU_DIR=$SCALFMM_STARPU_DIR -DSCALFMM_USE_FFT=ON -DFFT_DIR=$SCALFMM_FFT_DIR</code></pre></li>
<li><p>Configure (MKL BLAS/LAPACK and FFTW):</p>
<pre class="bash"><code>cmake .. -DSCALFMM_BUILD_DEBUG=OFF -DSCALFMM_USE_MPI=OFF -DSCALFMM_BUILD_TESTS=ON -DSCALFMM_BUILD_UTESTS=OFF -DSCALFMM_USE_BLAS=ON -DSCALFMM_USE_MKL_AS_BLAS=ON -DSCALFMM_USE_LOG=ON -DSCALFMM_USE_STARPU=ON -DSCALFMM_USE_CUDA=ON -DSCALFMM_USE_OPENCL=OFF -DHWLOC_DIR=$SCALFMM_HWLOC_DIR -DSTARPU_DIR=$SCALFMM_STARPU_DIR -DSCALFMM_USE_FFT=ON -DFFT_DIR=$SCALFMM_FFT_DIR</code></pre></li>
<li><p>Configure (MKL BLAS/LAPACK/FFT and No FFTW):</p></li>
</ul>
<blockquote>
<p>[Plafrim-Developers] Should use that one</p>
</blockquote>
<pre class="bash"><code>cmake .. -DSCALFMM_BUILD_DEBUG=OFF -DSCALFMM_USE_MPI=OFF -DSCALFMM_BUILD_TESTS=ON -DSCALFMM_BUILD_UTESTS=OFF -DSCALFMM_USE_BLAS=ON -DSCALFMM_USE_MKL_AS_BLAS=ON -DSCALFMM_USE_LOG=ON -DSCALFMM_USE_STARPU=ON -DSCALFMM_USE_CUDA=ON -DSCALFMM_USE_OPENCL=OFF -DHWLOC_DIR=$SCALFMM_HWLOC_DIR -DSTARPU_DIR=$SCALFMM_STARPU_DIR -DSCALFMM_USE_FFT=ON -DSCALFMM_USE_MKL_AS_FFTW=ON</code></pre>
<p>Valid-if:</p>
<pre><code>cmake .. ; if [[ &quot;$?&quot; == &quot;0&quot; ]] ; then echo &quot;OK&quot; ; fi</code></pre>
<h4 id="build">Build</h4>
<pre class="bash"><code>cd $SCALFMM_BUILD_DIR
make testBlockedUnifCudaBench</code></pre>
<p>Valid-if:</p>
<pre><code>ls ./Tests/Release/testBlockedUnifCudaBench ; if [[ &quot;$?&quot; == &quot;0&quot; ]] ; then echo &quot;OK&quot; ; fi</code></pre>
<h4 id="basic-executions">Basic Executions</h4>
<p>Information for scalfmm binaries</p>
<ul>
<li>Passing <code>--help</code> as parameter provide the possible/valid parameters</li>
<li>Simulation properties are choosen by :</li>
<li><code>-h</code> : height of the tree</li>
<li><code>-bs</code> : granularity/size of the group</li>
<li><code>-nb</code> : number of particles generated</li>
<li>Execution properties are choosen by the StarPU environment variables :</li>
<li><code>STARPU_NCPUS</code> : the number of CPU workers</li>
<li><code>STARPU_NCUDA</code> : the number of GPU workers (for heterogeneous binary)</li>
<li>By default the application will not compare the FMM interactions against the direct method (which is N^2) and so it is recommended to avoid the validation for large test cases. But to get the accuracy one must pass the parameter <code>-validation</code></li>
<li><code>-p2p-m2l-cuda-only</code> : to compute the P2P and the M2L only on GPU (the rest on the CPU)</li>
</ul>
<p>Examples:</p>
<pre><code>STARPU_NCPUS=3 STARPU_NCUDA=1 ./Tests/Release/testBlockedUnifCudaBench -nb 10000 -h 3</code></pre>
<ul>
<li>Visualize the execution trace (<strong>Optional</strong>)</li>
</ul>
<p>Convert the fxt file</p>
<pre class="bash"><code>$SCALFMM_STARPU_DIR/bin/starpu_fxt_tool -i &quot;/tmp/prof_file_&quot;$USER&quot;_0&quot;</code></pre>
<p>Then visualize the output with vite</p>
<pre class="bash"><code>vite ./paje.trace</code></pre>
<ul>
<li>Get execution times</li>
</ul>
<pre class="bash"><code>python $SCALFMM_STARPU_DIR/bin/starpu_trace_state_stats.py -t trace.rec</code></pre>
<p>Should give something like:</p>
<pre><code>&quot;Name&quot;,&quot;Count&quot;,&quot;Type&quot;,&quot;Duration&quot;
&quot;Initializing&quot;,3,&quot;Runtime&quot;,5.027746
&quot;Overhead&quot;,37,&quot;Runtime&quot;,0.110073
&quot;Idle&quot;,13,&quot;Other&quot;,0.03678
&quot;Scheduling&quot;,24,&quot;Runtime&quot;,16.529527
&quot;Sleeping&quot;,17,&quot;Other&quot;,2197.255516
&quot;FetchingInput&quot;,10,&quot;Runtime&quot;,0.012637
&quot;execute_on_all_wrapper&quot;,6,&quot;Task&quot;,8.431909
&quot;PushingOutput&quot;,10,&quot;Runtime&quot;,16.505568
&quot;P2P&quot;,1,&quot;Task&quot;,105.131112
&quot;Callback&quot;,4,&quot;Runtime&quot;,0.001048
&quot;Deinitializing&quot;,3,&quot;Runtime&quot;,0.014547
&quot;P2M&quot;,1,&quot;Task&quot;,2.543303
&quot;L2P&quot;,1,&quot;Task&quot;,5.649106
&quot;M2L-level-2&quot;,1,&quot;Task&quot;,2.167273</code></pre>
<p>Most of the script are in the addon directories</p>
<pre><code>export SCALFMM_AB=$SCALFMM_SOURCE_DIR/Addons/BenchEfficiency/</code></pre>
<h2 id="homogeneous-efficiencies">Homogeneous Efficiencies</h2>
<p>Here we compute the efficiencies for a given test case on CPU only.</p>
<p>Go in the build dir and create output dir</p>
<pre><code>cd $SCALFMM_BUILD_DIR
mkdir homogeneous</code></pre>
<p>Set up the configuration variables:</p>
<pre class="bash"><code>SCALFMM_NB=10000000
SCALFMM_H=7
SCALFMM_MIN_BS=100
SCALFMM_MAX_BS=3000
SCALFMM_MAX_NB_CPU=24</code></pre>
<p>Find best granularity in sequential and in parallel:</p>
<pre class="bash"><code>STARPU_NCPUS=1
STARPU_NCUDA=0
SCALFMM_BS_CPU_SEQ=`$SCALFMM_AB/scalfmmFindBs.sh &quot;./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs&quot; $SCALFMM_MIN_BS $SCALFMM_MAX_BS | $SCALFMM_AB/scalfmm_extract_key &quot;@BEST BS&quot; `
if [[ `which gnuplot | wc -l` == &quot;1&quot; ]] ;  then
    gnuplot -e &quot;filename=&#39;seq-bs-search&#39;&quot; $SCALFMM_AB/scalfmmFindBs.gplot
fi

STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=0
=`$SCALFMM_AB/scalfmmFindBs.sh &quot;./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs&quot; $SCALFMM_MIN_BS $SCALFMM_MAX_BS | $SCALFMM_AB/scalfmm_extract_key &quot;@BEST BS&quot; `
if [[ `which gnuplot | wc -l` == &quot;1&quot; ]] ;  then
    gnuplot -e &quot;filename=&#39;par-bs-search&#39;&quot; $SCALFMM_AB/scalfmmFindBs.gplot
fi</code></pre>
<p>Then we compute the efficiency using both granulirities and keep the .rec files.</p>
<pre class="bash"><code>source $SCALFMM_AB/execAllHomogeneous.sh</code></pre>
<p>We should end with all the rec files and their corresponding time files</p>
<pre class="bash"><code></code></pre>
<p>We compute the efficiencies</p>
<pre class="bash"><code></code></pre>
<p>We end with efficiency for the application and for the operators.</p>
<pre class="bash"><code></code></pre>
<p>We can plot each of them</p>
<pre class="bash"><code></code></pre>
<h2 id="generating-execution-results">Generating Execution Results</h2>
<p>For test case <code>-nb 10000000</code> (10 million) and <code>-h 6</code> (height of the tree equal to 6), we first want to know the best granularity <code>-bs</code>.</p>
<p>This parameter will certainly not be the same for sequential/parallel/heterogenous configurations.</p>
<pre class="bash"><code>SCALFMM_NB=10000000
SCALFMM_H=7
SCALFMM_MIN_BS=100
SCALFMM_MAX_BS=3000
SCALFMM_MAX_NB_CPU=24
SCALFMM_MAX_NB_GPU=4</code></pre>
<pre class="bash"><code>STARPU_NCPUS=1
STARPU_NCUDA=0
SCALFMM_BS_CPU_SEQ=`$SCALFMM_AB/scalfmmFindBs.sh -nb $SCALFMM_NB -h $SCALFMM_H $SCALFMM_MIN_BS $SCALFMM_MAX_BS | $SCALFMM_AB/scalfmm_extract_key &quot;@BEST BS&quot; `
if [[ `which gnuplot | wc -l` == &quot;1&quot; ]] ;  then
    gnuplot -e &quot;filename=&#39;seq-bs-search&#39;&quot; $SCALFMM_AB/scalfmmFindBs.gplot
fi

STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=0
SCALFMM_BS_CPU_PAR=`$SCALFMM_AB/scalfmmFindBs.sh -nb $SCALFMM_NB -h $SCALFMM_H $SCALFMM_MIN_BS $SCALFMM_MAX_BS | $SCALFMM_AB/scalfmm_extract_key &quot;@BEST BS&quot; `
if [[ `which gnuplot | wc -l` == &quot;1&quot; ]] ;  then
    gnuplot -e &quot;filename=&#39;par-bs-search&#39;&quot; $SCALFMM_AB/scalfmmFindBs.gplot
fi

STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=$SCALFMM_MAX_NB_GPU
SCALFMM_BS_CPU_GPU=`$SCALFMM_AB/scalfmmFindBs.sh -nb $SCALFMM_NB -h $SCALFMM_H $SCALFMM_MIN_BS $SCALFMM_MAX_BS | $SCALFMM_AB/scalfmm_extract_key &quot;@BEST BS&quot; `
if [[ `which gnuplot | wc -l` == &quot;1&quot; ]] ;  then
    gnuplot -e &quot;filename=&#39;cpugpu-bs-search&#39;&quot; $SCALFMM_AB/scalfmmFindBs.gplot
fi</code></pre>
<p>Then, we can execute three best configurations, and keep .rec for each of them:</p>
<pre class="bash"><code>STARPU_NCPUS=1
STARPU_NCUDA=0
./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_CPU_SEQ
SCALFMM_SEQ_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA.rec&quot;
mv trace.rec $SCALFMM_SEQ_REC

STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=0
./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_BS_CPU_PAR
SCALFMM_PAR_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA.rec&quot;
mv trace.rec $SCALFMM_PAR_REC

STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=$SCALFMM_MAX_NB_GPU
./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_BS_CPU_GPU
SCALFMM_PAR_CPU_GPU_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA.rec&quot;
mv trace.rec $SCALFMM_PAR_CPU_GPU_REC</code></pre>
<p>And we also want the GPU tasks only on GPU</p>
<pre class="bash"><code>STARPU_NCPUS=$SCALFMM_MAX_NB_CPU
STARPU_NCUDA=$SCALFMM_MAX_NB_GPU
./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_BS_CPU_GPU -p2p-m2l-cuda-only
SCALFMM_PAR_GPU_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA-GPUONLY.rec&quot;
mv trace.rec $SCALFMM_PAR_GPU_REC</code></pre>
<p>And we want the sequential version with parallel granularity:</p>
<pre class="bash"><code>STARPU_NCPUS=1
STARPU_NCUDA=0

./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_BS_CPU_PAR
SCALFMM_SEQ_CPU_BS_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA.rec&quot;
mv trace.rec $SCALFMM_SEQ_CPU_BS_REC

./Tests/Release/testBlockedUnifCudaBench -nb $SCALFMM_NB -h $SCALFMM_H -bs $SCALFMM_BS_CPU_GPU
SCALFMM_SEQ_GPU_BS_REC=&quot;trace-nb_$SCALFMM_NB-h_$SCALFMM_H-bs_$SCALFMM_CPU_SEQ-CPU_$STARPU_NCPUS-GPU_$STARPU_NCUDA.rec&quot;
mv trace.rec $SCALFMM_SEQ_GPU_BS_REC</code></pre>
<p>From these files, we are able to get the different efficencies.</p>
<h2 id="post-processing-and-plot">Post-processing and Plot</h2>
<p>From the file:</p>
<ul>
<li><code>$SCALFMM_SEQ_REC</code> : the resulting file from the sequential execution with best sequential granularity</li>
<li><code>$SCALFMM_PAR_REC</code> : the resulting file from a parallel execution (no GPU) with best parallel granularity</li>
<li><code>$SCALFMM_PAR_CPU_GPU_REC</code> : the resulting file from a parallel execution (hybrid) with best parallel-hybrid granularity</li>
<li><code>$SCALFMM_PAR_GPU_REC</code> : the resulting file with all possible tasks on GPU with best parallel-hybrid granularity</li>
<li><code>$SCALFMM_SEQ_CPU_BS_REC</code> : the resulting file from sequential execution with best parallel granularity</li>
<li><code>$SCALFMM_SEQ_GPU_BS_REC</code> : the resulting file from sequential execution with best parallel-hybrid granularity</li>
</ul>
<p>Getting all the efficency Solving the linear programming problem</p>
<p>Plotting the results</p>
<h2 id="automatization">Automatization</h2>
<pre class="bash"><code>SCALFMM_NB=10000000
SCALFMM_H=7
SCALFMM_MIN_BS=100
SCALFMM_MAX_BS=3000
SCALFMM_MAX_NB_CPU=24
SCALFMM_MAX_NB_GPU=4

scalfmm_generate_efficiency -nb $SCALFMM_NB -h $SCALFMM_H -start $SCALFMM_MIN_BS -end $SCALFMM_MAX_BS</code></pre>